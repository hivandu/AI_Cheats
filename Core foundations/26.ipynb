{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dufferent Optimizer 优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random(size=(10000, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = torch.from_numpy(np.random.uniform(0, 5, size=(10000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(in_features=8, out_features=1)\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "linear2 = torch.nn.Linear(in_features=1, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear, sigmoid, linear2).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1])\n",
      "torch.Size([10000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(model(train_x).shape)\n",
    "print(ytrue.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.9385, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.9333, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.9231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.9076, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8871, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8615, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.8308, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7950, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7543, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.7085, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6579, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.6023, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.5420, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4768, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.4070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.3325, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.2535, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.1700, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(11.0820, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.9897, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.8932, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.7926, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.6878, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.5792, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.4667, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.3504, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.2305, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(10.1072, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.9804, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.8503, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7172, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.5810, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.4419, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.3001, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.1558, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0089, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8598, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.7084, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.5551, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3999, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.2431, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.0847, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9249, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.7639, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6019, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.4390, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2753, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.1112, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9467, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7820, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4529, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2888, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1252, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.9623, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8004, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6396, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4800, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3219, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1655, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0110, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8584, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7081, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5603, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4150, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2725, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1329, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9966, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8635, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7340, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6082, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4863, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3684, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2548, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1455, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0408, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9408, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8457, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7557, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6708, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5912, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5171, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4486, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3858, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3288, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2778, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2328, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1940, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1614, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1352, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1153, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1020, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0951, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0948, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1010, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1139, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1334, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1596, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1924, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2318, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# yhat = model(train_x)\n",
    "# loss = loss_fn(yhat, ytrue)\n",
    "# print(loss)\n",
    "\n",
    "optimer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for e in range(100):\n",
    "    yhat = model(train_x)\n",
    "    loss = loss_fn(yhat, ytrue)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
