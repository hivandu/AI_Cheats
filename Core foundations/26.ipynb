{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dufferent Optimizer 优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random(size=(10000, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = torch.from_numpy(np.random.uniform(0, 5, size=(10000, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(in_features=8, out_features=1)\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "linear2 = torch.nn.Linear(in_features=1, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear, sigmoid, linear2).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1])\n",
      "torch.Size([10000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(model(train_x).shape)\n",
    "print(ytrue.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7308, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7290, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7254, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7200, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7127, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7037, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6929, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6803, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6660, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6499, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6320, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.6125, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5912, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5682, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5436, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5174, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4895, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4600, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4290, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3964, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3624, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3268, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2899, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2515, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.2117, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1706, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.1282, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0846, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0397, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9937, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9465, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8983, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.8490, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7987, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7475, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6954, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.6424, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5887, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5342, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4790, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4231, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3667, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3098, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.2524, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1946, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1364, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0780, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0193, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9604, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.9014, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8424, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7833, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.7243, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6655, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.6068, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5484, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4903, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.4326, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3753, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.3185, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2623, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.2067, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.1518, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0976, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.0442, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9918, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9402, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8897, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8402, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7918, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7446, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6986, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6539, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6105, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5686, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5281, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4891, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4516, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.4158, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3816, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3491, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.3184, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2894, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2623, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2371, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2138, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1924, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1730, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1557, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1404, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1271, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1160, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1070, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1002, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0955, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0930, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0927, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0946, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0987, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.1050, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# yhat = model(train_x)\n",
    "# loss = loss_fn(yhat, ytrue)\n",
    "# print(loss)\n",
    "\n",
    "optimer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for e in range(100):\n",
    "    yhat = model(train_x)\n",
    "    loss = loss_fn(yhat, ytrue)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    optimer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
